{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EashwarPrabu/gaied-lpmcai-main/blob/main/GenAi_LPMC_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DZNWqvfAZmxg",
        "outputId": "aef3cac3-5b4f-4695-fe69-1cc068b3b194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.44.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: pyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: jaydebeapi in /usr/local/lib/python3.11/dist-packages (1.2.3)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (1.15)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.11/dist-packages (from jaydebeapi) (1.5.2)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit transformers accelerate bitsandbytes xformers pyYAML jaydebeapi pdfplumber pypandoc python-docx bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o5QkDvEZxyT",
        "outputId": "d14d4ef7-333d-43de-af08-6f2129a20fbf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "-LB_Cht6aCab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo1.maven.org/maven2/com/h2database/h2/2.2.224/h2-2.2.224.jar -O h2.jar"
      ],
      "metadata": {
        "id": "_SB3ZwlBUQe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da608214-3cdb-484e-a350-6085932d477b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 14:30:33--  https://repo1.maven.org/maven2/com/h2database/h2/2.2.224/h2-2.2.224.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2614933 (2.5M) [application/java-archive]\n",
            "Saving to: ‘h2.jar’\n",
            "\n",
            "h2.jar              100%[===================>]   2.49M  2.29MB/s    in 1.1s    \n",
            "\n",
            "2025-03-26 14:30:35 (2.29 MB/s) - ‘h2.jar’ saved [2614933/2614933]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import email\n",
        "import hashlib\n",
        "import os\n",
        "import logging\n",
        "from email import policy\n",
        "from email.parser import BytesParser\n",
        "from email.message import Message\n",
        "import base64\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import yaml\n",
        "import ast\n",
        "import jaydebeapi\n",
        "import pdfplumber\n",
        "import pypandoc\n",
        "from docx import Document\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Setup H2 DB and store it in Streamlit session to ensure DB connections are not loaded up in every render of application\n",
        "def setup_h2_database():\n",
        "    if \"h2_connection\" not in st.session_state:\n",
        "        logger.info(\"Setting up H2 database...\")\n",
        "        try:\n",
        "            conn = jaydebeapi.connect(\n",
        "                \"org.h2.Driver\",\n",
        "                \"jdbc:h2:~/email_db\",\n",
        "                [\"sa\", \"\"],\n",
        "                \"h2.jar\"\n",
        "            )\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS emails (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    filename VARCHAR(255),\n",
        "                    sender VARCHAR(255),\n",
        "                    request_type VARCHAR(255),\n",
        "                    sub_request_type TEXT,\n",
        "                    confidence VARCHAR(255),\n",
        "                    email_hash VARCHAR(255)\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            # Store in session state to prevent reloading\n",
        "            st.session_state.h2_connection = conn\n",
        "            st.session_state.h2_cursor = cursor\n",
        "            logger.info(\"H2 database setup complete.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error setting up H2 database: {str(e)}\")\n",
        "            st.session_state.h2_connection = None\n",
        "            st.session_state.h2_cursor = None\n",
        "\n",
        "    return st.session_state.h2_connection, st.session_state.h2_cursor\n",
        "\n",
        "# Load and cache the model using Streamlit cache resource annotation to ensure app doesn't reload during every render\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,  # Use 4-bit quantization\n",
        "        bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation\n",
        "        bnb_4bit_quant_type=\"nf4\",  # More efficient quantization type\n",
        "        bnb_4bit_use_double_quant=True  # Extra compression\n",
        "    )\n",
        "\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    logger.info(f\"Loading model from cache...\")\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model()\n",
        "\n",
        "# Parse the configuration (request, sub-request and keyword) that will be passed as prompt to LLM\n",
        "def config_parser():\n",
        "  logger.info(f\"Parsing configuration from yml file...\")\n",
        "  with open(\"DetectionTypes.yml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "  return config\n",
        "\n",
        "config = config_parser()\n",
        "request_types = [ request for request in config.keys()]\n",
        "\n",
        "# Prompt Template for the classification task\n",
        "classification_prompt = \"\"\"\n",
        "You are a expert helping users to categorize the text based on predefined classes. Only categorize the content based on the available list of classes.\n",
        "Given an input text, classify it into one of the **main request types**: {request_types} and, if applicable, further classify it into a **sub-request** type.\n",
        "Your response **must only** be a JSON dictionary with: {outputFormat}\n",
        "\n",
        "### **Classification Rules**:\n",
        "- **Request Type:** Match the text to the most relevant **Request type** based on its semantics, meaning and the sample list of keywords.\n",
        "- **Sub-Request Type:** If the text aligns with a sub-category, classify it accordingly.\n",
        "- The following are some of the example Request-Types, their sample reference keywords and Sub-Request Types.\n",
        "{config}\n",
        "\n",
        "Text: \"{input_text}\"\n",
        "\n",
        "Output:\n",
        "\"\"\"\n",
        "# Prompt Template for the Name-Entity Recognition task to identify the key-value pairs\n",
        "ner_prompt = \"\"\"\n",
        "You are a financial expert who is responsible identifying all the important key value pairs associated with Finance/Banking, etc.\n",
        "Extract financial details from the given content and return them **strictly as a dictionary of key-value pairs**.\n",
        "Text: \"{input_text}\"\n",
        "\n",
        "Output:\n",
        "\"\"\"\n",
        "# Structure of Classification output\n",
        "classificationOutputFormat = {\n",
        "    \"output\": {\n",
        "    \"requestType\": \"string\",\n",
        "    \"subRequestTypes\": [\"string\"],\n",
        "    \"probability\": \"float\",\n",
        "    \"reasoning\": \"string\"\n",
        "  }\n",
        "}\n",
        "\n",
        "# Tokenizes the input text (prompt and the email content) into tensors and invokes the model\n",
        "# Decodes the tensors back to string and returns the response\n",
        "def invoke(prompt):\n",
        "  # Ensure pad_token is properly set\n",
        "  if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Assign pad_token explicitly\n",
        "\n",
        "  # Tokenize input with attention mask\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "  attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "  # Generate output with explicit attention mask\n",
        "  output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,  # Pass attention_mask explicitly\n",
        "    max_length=4096,\n",
        "    pad_token_id=tokenizer.pad_token_id  # Use correct pad_token_id\n",
        "  )\n",
        "\n",
        "  # Decode and Extract JSON Output\n",
        "  response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  # Extract only the JSON output\n",
        "  if \"Output:\" in response:\n",
        "      response = response.split(\"Output:\")[-1].strip()\n",
        "\n",
        "  return response\n",
        "\n",
        "# Classify takes content of mail as parameter, creates the classification prompt and then calls invoke function to get LLM response\n",
        "def classify(input_text):\n",
        "    formatted_prompt = classification_prompt.format(\n",
        "        input_text=input_text,\n",
        "        outputFormat=classificationOutputFormat,\n",
        "        request_types=request_types,\n",
        "        config=config)\n",
        "    classifiedOutput = invoke(formatted_prompt)\n",
        "    logger.info(f\"Classified results: {classifiedOutput}\")\n",
        "    return ast.literal_eval(classifiedOutput)\n",
        "\n",
        "# Extract takes content of mail as parameter, creates the NER prompt and then calls invoke function to get LLM response\n",
        "def extract(input_text):\n",
        "    formatted_prompt = ner_prompt.format(input_text=input_text)\n",
        "    extractedOutput = invoke(formatted_prompt)\n",
        "    logger.info(f\"Extracted results: {extractedOutput}\")\n",
        "    return json.loads(extractedOutput)\n",
        "\n",
        "# Wrapper for Classify and Extract methods, responsible for concatenating all the output\n",
        "def classify_and_extract(input_text):\n",
        "    classifiedResult = classify(input_text)\n",
        "    extractedResult = extract(input_text)\n",
        "    logger.info(f\"Final classification results: {classifiedResult}\")\n",
        "    logger.info(f\"Final extraction results: {extractedResult}\")\n",
        "\n",
        "    classifiedResult[\"nameValuePairs\"] = extractedResult\n",
        "    print(classifiedResult)\n",
        "    return classifiedResult\n",
        "\n",
        "# Directory to store attachments\n",
        "ATTACHMENTS_DIR = \"attachments/\"\n",
        "os.makedirs(ATTACHMENTS_DIR, exist_ok=True)\n",
        "\n",
        "# Helper functions to parse the attachments based on their file type\n",
        "def process_attachment(file_path):\n",
        "    \"\"\"Reads and extracts text from PDF, DOCX, DOC, and TXT files.\"\"\"\n",
        "    try:\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            return extract_text_from_pdf(file_path)\n",
        "        elif file_path.endswith(\".docx\"):\n",
        "            return extract_text_from_docx(file_path)\n",
        "        elif file_path.endswith(\".doc\"):\n",
        "            return extract_text_from_doc(file_path)\n",
        "        elif file_path.endswith(\".txt\"):\n",
        "            return extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            logger.warning(f\"Unsupported file type: {file_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing attachment {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "        return text if text else \"(No readable text in PDF)\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    \"\"\"Extracts text from a DOCX file.\"\"\"\n",
        "    try:\n",
        "        doc = Document(docx_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text if text else \"(No readable text in DOCX)\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading DOCX {docx_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    \"\"\"Extracts text from a DOC file using pypandoc.\"\"\"\n",
        "    try:\n",
        "        return pypandoc.convert_file(doc_path, \"plain\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading DOC {doc_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_txt(txt_path):\n",
        "    \"\"\"Extracts text from a TXT file.\"\"\"\n",
        "    try:\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read().strip()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading TXT {txt_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Parses the file uploaded through UI.\n",
        "# Walks through all the email content and its attachment and returns them\n",
        "def parse_eml(file):\n",
        "    try:\n",
        "        msg: Message = BytesParser(policy=policy.default).parse(file)\n",
        "        logger.info(\"Email content classifier\")\n",
        "\n",
        "        subject = msg[\"subject\"] or \"No Subject\"\n",
        "        sender = msg[\"from\"]\n",
        "        recipient = msg[\"to\"]\n",
        "        date = msg[\"date\"]\n",
        "        body = \"\"\n",
        "        attachments = []\n",
        "\n",
        "        # Extract body and attachments\n",
        "        for part in msg.walk():\n",
        "            content_type = part.get_content_type()\n",
        "\n",
        "            if content_type == \"text/plain\":\n",
        "                body += part.get_payload(decode=True).decode(errors=\"ignore\")\n",
        "            elif content_type == \"text/html\":\n",
        "                html_content = part.get_payload(decode=True).decode(errors=\"ignore\")\n",
        "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "                body += soup.get_text(separator=\" \") + \"\\n\"\n",
        "            elif part.get_content_disposition() == \"attachment\":\n",
        "                attachment_name = part.get_filename()\n",
        "                attachment_data = part.get_payload(decode=True)\n",
        "\n",
        "                if not attachment_name or not attachment_data:\n",
        "                    continue  # Skip if no valid attachment\n",
        "\n",
        "                # Save the attachment file locally\n",
        "                file_path = os.path.join(ATTACHMENTS_DIR, attachment_name)\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    f.write(attachment_data)\n",
        "\n",
        "                # Try to extract text from known file types\n",
        "                extracted_text = process_attachment(file_path)\n",
        "\n",
        "                if extracted_text:\n",
        "                    attachment_type = \"text\"\n",
        "                    attachment_content = extracted_text  # Store extracted text\n",
        "                else:\n",
        "                    attachment_type = \"binary\"\n",
        "                    attachment_content = base64.b64encode(attachment_data).decode()  # Store Base64\n",
        "\n",
        "                logger.info(f\"Attachment found: {attachment_name} (Type: {attachment_type}, Size: {len(attachment_data)} bytes)\")\n",
        "\n",
        "                attachments.append({\n",
        "                    \"name\": attachment_name,\n",
        "                    \"type\": attachment_type,\n",
        "                    \"content\": attachment_content,  # Extracted text or Base64\n",
        "                    \"size\": len(attachment_data)\n",
        "                })\n",
        "\n",
        "        email_data = {\n",
        "            \"subject\": subject,\n",
        "            \"from\": sender,\n",
        "            \"to\": recipient,\n",
        "            \"date\": date,\n",
        "            \"body\": body,\n",
        "            \"attachments\": attachments  # List of extracted attachments\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Email components extracted: {subject}\")\n",
        "        return email_data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error parsing email: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Computes the hash using the email sender and email content\n",
        "def compute_hash(email_data):\n",
        "    text = email_data['subject'] + email_data['body']\n",
        "    email_hash = hashlib.md5(text.encode()).hexdigest()\n",
        "    logger.info(f\"Hash computed: {email_hash}\")\n",
        "\n",
        "# Detects duplicates from DB using email sender and email hash\n",
        "def detect_duplicates(cursor, email_data, email_hash):\n",
        "    logger.info(f\"Detecting duplicates for email: {email_data['filename']}\")\n",
        "    try:\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM emails WHERE sender = ? and email_hash = ?\", (email_data['from'], email_hash,))\n",
        "        count = cursor.fetchone()[0]\n",
        "        is_duplicate = count > 0\n",
        "        logger.info(f\"Duplicate check: {'Duplicate' if is_duplicate else 'Not a duplicate'}\")\n",
        "        return is_duplicate\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error detecting duplicates for {email_data['filename']}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Store the results for an email in H2 DB\n",
        "def store_email_data(cursor, email_data, output, email_hash):\n",
        "    logger.info(f\"Storing email data for: {email_data['filename']}\")\n",
        "    try:\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO emails (filename, sender, request_type, sub_request_type, confidence, email_hash) VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            email_data['filename'],\n",
        "            email_data['from'],\n",
        "            output['result']['output']['requestType'],\n",
        "            json.dumps(output['result']['output']['subRequestTypes']),\n",
        "            output['result']['probability'],\n",
        "            email_hash\n",
        "        ))\n",
        "        logger.info(\"Email data stored successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error storing email data for {email_data['filename']}: {str(e)}\")\n",
        "\n",
        "# Streamlit app title\n",
        "st.title(\"GenAI LPMC - Email content classifier\")\n",
        "uploaded_file = st.file_uploader(\"Upload an .eml file\", type=[\"eml\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    with uploaded_file:\n",
        "        filename = uploaded_file.name\n",
        "        uploaded_file.seek(0)  # Reset file pointer before reading\n",
        "        email_data = parse_eml(uploaded_file)\n",
        "        print(\"Updated contents\")\n",
        "        print(email_data)\n",
        "        email_data[\"filename\"] = filename\n",
        "        email_hash = compute_hash(email_data)\n",
        "        conn, cursor = setup_h2_database()\n",
        "        if conn and cursor:\n",
        "            isDuplicate = detect_duplicates(cursor, email_data, email_hash)\n",
        "            if isDuplicate:\n",
        "                logger.error(f\"Dupicate detected. Mail contents already exists in DB\")\n",
        "            else:\n",
        "                with st.spinner(f\"Processing email {filename}... Please wait ⏳\"):\n",
        "                    attachment_data = \"\"\n",
        "                    for attachment in email_data[\"attachments\"]:\n",
        "                        if attachment[\"type\"] == \"text\":\n",
        "                            attachment_data += \"\\n\\n--- Attachment: {} ---\\n{}\".format(attachment[\"name\"], attachment[\"content\"])\n",
        "                    result = classify_and_extract(email_data[\"body\"] + attachment_data)\n",
        "                    store_email_data(cursor, email_data, result, email_hash)\n",
        "                    conn.commit()\n",
        "                    output = {\n",
        "                        \"emailContents\": email_data,\n",
        "                        \"result\": result\n",
        "                    }\n",
        "                    st.json(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUwOXWpNaC44",
        "outputId": "0ba7d8fb-0863-410d-b189-d7673a2f5bd8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AwHpDVyYbGGt",
        "outputId": "68c51945-1a1f-4671-c79c-5c2681e0df4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Hackathon-25` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Hackathon-25`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YTs8-XDraYL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}